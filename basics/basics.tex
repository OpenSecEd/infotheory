%\documentclass[handout]{beamer}
\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[swedish,british]{babel}
\usepackage{url}
\usepackage{graphicx}
\usepackage{color}
\usepackage{subfig}
\usepackage{multicol}
\usepackage{amssymb,amsmath,amsthm}
\usepackage{booktabs}
%\usepackage[squaren,binary]{SIunits}
\usepackage[binary-units]{siunitx}
\usepackage[strict]{csquotes}
\usepackage{cleveref}
\usepackage{hhcount}

\setbeamertemplate{bibliography item}[text]
\usepackage[natbib,style=alphabetic,maxbibnames=99]{biblatex}
\addbibresource{basics.bib}

\usepackage{xparse}
\ProvideDocumentEnvironment{exercise}{o}{%
  \setbeamercolor{block body}{bg=yellow!30,fg=black}
  \setbeamercolor{block title}{bg=yellow,fg=black}
  \IfValueTF{#1}{%
    \begin{block}{Exercise: #1}
  }{%
    \begin{block}{Exercise}
  }
}{%
  \end{block}
}
\ProvideDocumentEnvironment{remark}{o}{%
  \IfValueTF{#1}{%
    \begin{alertblock}{Remark: #1}
  }{%
    \begin{alertblock}{Remark}
  }
}{%
  \end{alertblock}
}
\DeclareMathOperator{\powerset}{\mathcal{P}}
\DeclareMathOperator{\p}{\mathcal{P}}
\let\P\p
\DeclareMathOperator{\C}{\mathcal{C}}
\DeclareMathOperator{\K}{\mathcal{K}}
\DeclareMathOperator{\E}{\mathcal{E}}
\DeclareMathOperator{\D}{\mathcal{D}}

\DeclareMathOperator{\N}{\mathbb{N}}
\DeclareMathOperator{\Z}{\mathbb{Z}}
\DeclareMathOperator{\R}{\mathbb{R}}

\let\stoch\mathbf{}

\DeclareMathOperator{\xor}{\oplus}

\renewcommand{\qedsymbol}{Q.E.D.}

\mode<presentation>{%
  \usetheme{Berlin}
  \setbeamercovered{transparent}
}
\setbeamertemplate{footline}{\insertframenumber}

\title{%
  Applied Information Theory
}
\author{%
  Daniel Bosk
}
\institute[MIUN IKS]{%
  Department of Information and Communication Systems,\\
  Mid Sweden University, Sundsvall.
}
\date{\today}

\AtBeginSection[]{%
  \begin{frame}
    \tableofcontents[currentsection]
  \end{frame}
}

\begin{document}

\begin{frame}
  \titlepage{}
\end{frame}

\begin{frame}
  \tableofcontents
\end{frame}


% Since this a solution template for a generic talk, very little can
% be said about how it should be structured. However, the talk length
% of between 15min and 45min and the theme suggest that you stick to
% the following rules:  

% - Exactly two or three sections (other than the summary).
% - At *most* three subsections per section.
% - Talk about 30s to 2min per frame. So there should be between about
%   15 and 30 frames, all told.


\section{Introduction}

\subsection{History}

\begin{frame}
  \begin{itemize}
    \item Created 1948 by Shannon's paper 
      \citetitle{Shannon1948amt}~\cite{Shannon1948amt}.

      \pause{}

    \item He starts using the term \enquote{entropy} as a measure for 
      information.
      \begin{itemize}
        \item In physics entropy measures the disorder of molecules.
        \item Shannon's entropy measures disorder of information.
      \end{itemize}

      \pause{}

    \item He used this theory to analyse communication.
      \begin{itemize}
        \item What are the theoretical limits for different channels?
        \item How much redundancy is needed for certain noise?
      \end{itemize}

  \end{itemize}
\end{frame}

\begin{frame}
  \begin{itemize}
    \item This theory is interesting on the physical layer of networking.

      \pause{}

    \item It's also interesting for security.
      \begin{itemize}
        \item Field of Information Theoretic Security
        \item \enquote{Efficiency} of passwords
        \item Measure identifiability
        \item \dots
      \end{itemize}
  \end{itemize}
\end{frame}


\input{probability.tex}


\section{Shannon entropy}

\subsection{Definition of Shannon Entropy}

\begin{frame}
  \begin{definition}[Shannon entropy]
    \begin{itemize}
      \item Stochastic variable \(\stoch X\) assumes values from \(X\).
      \item Shannon entropy \(H(\stoch X)\) defined as
        \begin{align*}
          H(\stoch X) = -K \sum_{x\in X} \Pr(\stoch X = x)\log \Pr(\stoch X = x),
        \end{align*}
      \item Usually \(K = \frac{1}{\log 2}\) to give entropy in unit bits  
        (\si{\bit}).
    \end{itemize}
  \end{definition}
\end{frame}

\begin{frame}
  \begin{block}{Shannon entry can be seen as \dots}
    \begin{itemize}
      \item \dots how much choice in each event.

      \item \dots the uncertainty of each event.

      \item \dots how many bits to store each event.

      \item \dots how much information it produces.

    \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
  \begin{example}[Toss a coin]
    \begin{itemize}
      \item Stochastic variable \(\stoch{S}\) takes values from \(S = \{h, 
          t\}\).
      \item We have \(\Pr(\stoch S = h) = \Pr(\stoch S = t) = \frac{1}{2}.\)
      \item This gives \(H(\stoch S)\) as follows:
        \begin{align*}
          H(\stoch S) &= -\left( \Pr(\stoch S = h)\log \Pr(\stoch S = h) 
            + \Pr(\stoch S = t) \log \Pr(\stoch S = t) \right) \\
          &= -2\times \frac{1}{2}\log \frac{1}{2} = \log 2 = 1.
        \end{align*}
    \end{itemize}
  \end{example}
\end{frame}

\begin{frame}
  \begin{example}[Roll a die]
    \begin{itemize}
      \item Stochastic variable \(\stoch D\) takes values from \(D 
          = \{\fcdice{1}, \fcdice{2}, \fcdice{3}, \fcdice{4}, \fcdice{5}, 
          \fcdice{6}\}\).
      \item We have \(\Pr(\stoch D = d) = \frac{1}{6}\) for all \(d\in D\).
      \item The entropy \(H(\stoch D)\) is as follows:
        \begin{align*}
          H(\stoch D) &= -\sum_{d\in D} \Pr(\stoch D = d)\log\Pr(\stoch D = d) \\
          &= -6\times \frac{1}{6}\log\frac{1}{6} = \log 6 \approx 2.585.
        \end{align*}
    \end{itemize}
  \end{example}
\end{frame}

\begin{frame}
  \begin{remark}
    \begin{itemize}
      \item If we didn't know already, we now know that a roll of a die \dots
        \begin{itemize}
          \item contains more \enquote{choice} than a coin toss.
          \item is more uncertain to predict than a coin toss.
          \item requires more bits to store than a coin toss.
          \item produces more information than a coin toss.
        \end{itemize}

      \item What if we modify the die a bit?
    \end{itemize}
  \end{remark}
\end{frame}

\begin{frame}
  \begin{example}[Roll of a modified die]
    \begin{itemize}
      \item Stochastic variable \(D'\) taking values from \(D\).
      \item We now have \(\Pr(\stoch D' = \fcdice{6}) = \frac{9}{10}\) and 
        \(\Pr(\stoch D' = d) = \frac{1}{10}\times\frac{1}{5}\) for \(d\neq 
          \fcdice{6}\).
      \item This yields
        \begin{align*}
          H(\stoch D') &= -\left( \frac{9}{10}\log\frac{9}{10} + \sum_{d\neq 6} 
            \frac{1}{50}\log\frac{1}{50} \right) \\
          &= -\frac{9}{10}\log\frac{9}{10} -5\times\frac{1}{50}\log\frac{1}{50} 
          \\
          &= -\frac{9}{10}\log\frac{9}{10} -\frac{1}{10}\log\frac{1}{50} \approx 
          0.486.
        \end{align*}
    \end{itemize}
  \end{example}
\end{frame}

\begin{frame}
  \begin{remark}
    \begin{itemize}
      \item This die is much easier to predict.
      \item It produces much less information --- less than a coin toss!
      \item Requires less data for storage etc.
    \end{itemize}
  \end{remark}
\end{frame}

\subsection{Properties for Shannon entropy}

\begin{frame}
  \begin{definition}
    \begin{itemize}
      \item Function \(f\colon \R\to \R\) such that
        \begin{align*}
          tf(x) + (1-t)f(y) \leq f(tx + (1-t)y),
        \end{align*}

      \item Then \(f\) is \emph{concave}.
      \item With strict inequality for \(x\neq y\) we say that \(f\) is 
        \emph{strictly concave}.
    \end{itemize}
  \end{definition}

  \begin{example}
    \(\log\colon \R\to \R\) is strictly concave.
  \end{example}
\end{frame}

%\begin{frame}
%  \begin{lemma}
%    Låt \(f\) vara en strikt konkav funktion.
%    Då har vi att
%    \begin{align*}
%      tf(x) + (1-y)f(y) = f( tx + (1-t)y )
%    \end{align*}
%    om och endast om \(x = y\).
%  \end{lemma}
%\end{frame}
%
%\begin{frame}
%  \begin{proof}
%    Antag \(x = y\).
%    Då har vi
%    \begin{align*}
%      tf(x) + (1-t)f(x) = f(x)(t+1-t) = f(x).
%    \end{align*}
%    Men
%    \begin{align*}
%      f(tx + (1-t)x) = f((t+1-t)x) = f(x)
%    \end{align*}
%    och alltså har vi likhet.
%
%    Antag \(tf(x) + (1-t)f(y) = f(tx + (1-t)y)\).
%    Då har vi
%    \begin{align*}
%      t( f(x) - f(y) ) + f(y) = f( t( x - y ) + y ).
%    \end{align*}
%    Högerledet implicerar \(f(x) = f(y)\), men då beror vänterledet enbart på 
%    \(f(y)\) och således måste även \(x = y\).
%  \end{proof}
%\end{frame}

\begin{frame}
  \begin{theorem}[Jensen's inequality]
    \begin{itemize}
      \item Strictly concave function \(f\colon \R\to \R\).
      \item Real numbers \(a_1, a_2,\ldots, a_n > 0\) such that \(\sum_{i=1}^n 
          a_i = 1\).
      \item Then we have
        \begin{align*}
          \sum_{i=1}^n a_i f(x_i) \leq f\left( \sum_{i=1}^n a_i x_i\right).
        \end{align*}
      \item We have equality iff \(x_1 = x_2 = \cdots = x_n\).
    \end{itemize}
  \end{theorem}
\end{frame}

%\begin{frame}
%  \begin{block}{Bevis för Jensens olikhet.}
%    Bevis genom induktion.
%    Antag \(n=2\).
%    Då har vi att \(a_1 + a_2 = 1\) och alltså \(a_1 = 1 - a_2\).
%    Eftersom \(f\) är konkav har vi att
%    \begin{align*}
%      a_1f(x_1) + a_2f(x_2) \leq f(a_1x_1 + a_2x_2).
%    \end{align*}
%
%    Antag sant för \(n=k\).
%    \begin{align}
%      \label{eq:JensenIndhyp}
%      \sum_{i=1}^k a_i = 1 \land
%        \sum_{i=1}^k a_if(x_i) \leq f\left( \sum_{i=1}^k a_ix_i\right).
%    \end{align}
%  \end{block}
%\end{frame}
%
%\begin{frame}
%  \begin{proof}[Forts. bevis för Jensens olikhet]
%    Lås oss visa att detta även gäller för \(n=k+1\).
%    Eftersom att \(f\) är konkav gäller att
%    \begin{align}
%      \label{eq:JensenInductive1}
%      \sum_{i=1}^{k+1} a_if(x_i) &= a_1f(x_1) + (1-a_1)\sum_{i=2}^{k+1} 
%      \frac{a_i}{1-a_1}f(x_i) \\
%      \label{eq:JensenInductive2}
%        &\leq f\left(a_1x_1 + (1-a_i)\sum_{i=2}^{k+1} \frac{a_i}{1-a_1} 
%        x_i\right).
%    \end{align}
%    Då \(\sum_{i=2}^{k+1}\frac{a_i}{1-a_i} = 1\) kan vi tillämpa 
%    induktionshypotesen \eqref{eq:JensenIndhyp} i \eqref{eq:JensenInductive1} 
%    och \eqref{eq:JensenInductive2}, alltså är det sant för alla \(n\in \N\).
%
%    Vidare följer likhet från lemmat ovan.
%  \end{proof}
%\end{frame}

\begin{frame}
  \begin{theorem}
    \begin{itemize}
      \item Stochastic variable \(\stoch X\) with probability distribution 
        \begin{equation*}
          p_1, p_2,\ldots, p_n, \text{ where } p_i > 0 \text{ for } 1\leq i\leq 
          n.
        \end{equation*}
      \item Then \(H(\stoch X)\leq \log n\).
      \item Equality iff \(p_1 = p_2 = \cdots = p_n = 1/n\).
    \end{itemize}
  \end{theorem}
\end{frame}

\begin{frame}
  \begin{proof}
    The theorem follows directly from Jensen's inequality:
    \begin{align*}
      H(\stoch X) &= -\sum_{i=1}^n p_i\log p_i = \sum_{i=1}^n 
      p_i\log\frac{1}{p_i} \\
      &\leq \log\sum_{i=1}^n p_i\frac{1}{p_i} = \log n.
    \end{align*}
    With equality iff \(p_1 = p_2 = \cdots = p_n\).
  \end{proof}
\end{frame}

\begin{frame}
  \begin{corollary}
    \(H(\stoch X) = 0\) iff \(\Pr(\stoch X = x) = 1\) for some \(x\in X\) and 
    \(\Pr(\stoch X = x^\prime) = 0\) for all \(x\neq x^\prime \in X\).
  \end{corollary}

  \begin{proof}
    \begin{itemize}
      \item If \(\Pr(\stoch X = x) = 1\), then \(n = 1\) and thus \(H(\stoch X) 
          = \log n = 0\).

      \item If \(H(\stoch X) = 0\), then \(H(\stoch X) \leq \log n = 0\).
        Thus \(n = 1\).
    \end{itemize}
  \end{proof}
\end{frame}

%\begin{frame}
%  \begin{theorem}
%    Följande egenskaper gäller:
%    \begin{enumerate}
%      \item\label{prop:cont} \(H\) är kontinuerlig.
%      \item\label{prop:mono} Om \(\Pr(\stoch X = x) = 1/|X|\) för alla \(x\in 
%        X\) då är \(H\) en monotont stigande funktion med avseende på \(|X|\).
%    \end{enumerate}
%  \end{theorem}
%
%  \begin{proof}
%    \Cref{prop:cont} följer direkt av att logaritmen är kontinuerlig och 
%    funktionssammansättningar av kontinuerliga funktioner är kontinuerliga.
%
%    \Cref{prop:mono} följer av föregående sats.
%  \end{proof}
%\end{frame}

\begin{frame}
  \begin{lemma}
    \begin{itemize}
      \item Stochastic variables \(\stoch X\) and \(\stoch Y\).
      \item Then we have
        \begin{align*}
          H(\stoch X, \stoch Y)\leq H(\stoch X) + H(\stoch Y).
        \end{align*}
      \item Equality iff \(\stoch X\) and \(\stoch Y\) are independent.
    \end{itemize}
  \end{lemma}
\end{frame}

%\begin{frame}
%  \begin{proof}
%  \end{proof}
%\end{frame}
%
%\begin{frame}
%  \begin{theorem}
%    En utjämning av sannolikheterna ökar \(H(\stoch X)\).
%  \end{theorem}
%\end{frame}
%
%\begin{frame}
%  \begin{proof}
%  \end{proof}
%\end{frame}

\subsection{Conditional entropy}

\begin{frame}
  \begin{definition}[Conditional entropy]
    \begin{itemize}
      \item Define \emph{conditional entropy} \(H(\stoch Y\mid \stoch X)\) 
        as
        \begin{align*}
          H(\stoch Y\mid \stoch X) = %-\sum_{i,j} \Pr(i,j)\log \Pr(j\mid i)
          -\sum_y\sum_x \Pr(\stoch Y = y)\Pr(\stoch X = x\mid y)\log \Pr(\stoch 
          X = x\mid y).
        \end{align*}
    \end{itemize}
  \end{definition}

  \pause{}

  \begin{remark}
    This is the uncertainty in \(\stoch Y\) which is not revealed by \(\stoch 
      X\).
  \end{remark}
\end{frame}

\begin{frame}
  \begin{theorem}
    \(H(\stoch X, \stoch Y) = H(\stoch Y) + H(\stoch X\mid \stoch Y)\)
  \end{theorem}
%\end{frame}
%
%\begin{frame}
%  \begin{proof}
%  \end{proof}
%\end{frame}
%
%\begin{frame}
  \begin{corollary}
    \(H(\stoch X\mid \stoch Y) \leq H(\stoch X)\).
  \end{corollary}

  \begin{corollary}
    \(H(\stoch X\mid \stoch Y) = H(\stoch X)\) iff \(\stoch X\) and \(\stoch 
      Y\) independent.
  \end{corollary}
\end{frame}
%
%\begin{frame}
%  \begin{proof}
%  \end{proof}
%\end{frame}
%
%\begin{frame}
%  \begin{theorem}
%    Entropin för en Markovprocess
%  \end{theorem}
%\end{frame}

\subsection{Informationstäthet och redundans}

\begin{frame}
  \begin{definition}
    \begin{itemize}
      \item Natural language \(L\).
      \item Stochastic variable \(\stoch P^n_L\) of strings of length \(n\).
      \item Entropy of \(L\) defined as
        \begin{align*}
          H_L = \lim_{n\to \infty}\frac{H(\stoch P^n_L)}{n}.
        \end{align*}
      \item Redundancy in \(L\) is
        \begin{align*}
          R_L = 1 - \frac{H_L}{\log |P_L|}.
        \end{align*}
    \end{itemize}
  \end{definition}
\end{frame}

\begin{frame}
  \begin{remark}
    Meaning we have \(H_L\) bits per character in \(L\).
  \end{remark}

  \begin{example}[\cite{Shannon1948amt}]
    \begin{itemize}
      \item Entropy of 1--1.5 bits per character in English.
      \item Redundancy of approximately \(1 - \frac{1.25}{\log 26} = 0.61\).
    \end{itemize}
  \end{example}

\end{frame}

\begin{frame}
  \begin{example}[\cite{Shannon1948amt}]
    Two-dimensional cross-word puzzles requires redundancy of approximately 
    \(0.5\).
  \end{example}

  \begin{example}
    \begin{itemize}
      \item Redundancy of \enquote{SMS languages} is lower than for 
        \enquote{non-SMS languages}.

      \item Compare \enquote{också} and \enquote{oxå}.

    \end{itemize}
  \end{example}

  \begin{remark}
    \begin{itemize}
      \item Lower redundancy is more space-efficient.
      \item Incurs more errors.
    \end{itemize}
  \end{remark}
\end{frame}

%\begin{frame}
%  \begin{itemize}
%    \item Detta säger också att vi kan uppskatta entropin för en given 
%      Markovprocess.
%
%    \item Shannon modellerade språket som en Markovprocess i sin artikel 
%      \cite{Shannon1948amt}.
%
%    \item Vi kan även beräkna entropin för ett givet tillstånd i en 
%      Markovprocess genom betingad entropi.
%
%  \end{itemize}
%\end{frame}

\subsection{Information gain}

\begin{frame}
  \begin{definition}
    \begin{itemize}
      \item Set \(U\) of possible outcomes.
      \item Probability of outcome \(u\in U\) denoted \(p_u\).
      \item We learn that some \emph{unknown} outcome is in \(A\subset U\).
      \item Then the \emph{information gain} \(G(A\mid U)\) is defined as
        \begin{align*}
          G(A\mid U) = \log\frac{1}{\Pr(A)} = -\log\Pr(A),
        \end{align*}
        where \(\Pr(A) = \sum_{i\in A} p_i\).
    \end{itemize}
  \end{definition}
\end{frame}

\begin{frame}
  \begin{example}[Roll of dice again]
    \begin{itemize}
      \item Someone rolls and we should guess the result, \(\frac{1}{6}\) 
        chance.
      \item We learn that it was an even number, we gain
        \begin{align*}
          -\log\left( \frac{1}{6} + \frac{1}{6} + \frac{1}{6}\right) =
          -\log\frac{3}{6} = \log\frac{6}{3} = \log 2 = 1.
        \end{align*}
      \item The remaining uncertainty is \SI{1.58}{\bit}.
    \end{itemize}
  \end{example}

  \pause{}

  \begin{remark}
    \begin{itemize}
      \item \(X' = \{\fcdice{1}, \fcdice{3}, \fcdice{5}\}\)
      \item \(H(\stoch X') = - \sum_{x\in X'} \Pr(\stoch X' = x)\log \Pr(\stoch 
          X' = x)\)
      \item I.e.\ \(- 3 \times \frac{1}{3}\log\frac{1}{3} \approx 1.58\).
    \end{itemize}
  \end{remark}
\end{frame}

\begin{frame}
  \begin{example}[Dice yet again]
    \begin{itemize}
      \item We learn the die show less than five.
      \item This yields
        \begin{align*}
          -\log\left( 5\times\frac{1}{6}\right) = \log\frac{6}{5}\approx 0.26
        \end{align*}
    \end{itemize}
  \end{example}
\end{frame}


\section[Tillämpningar]{Tillämpningar inom säkerhet}

\subsection{Lösenord}

\begin{frame}
  \begin{block}{Idé~\cite{Komanduri2011opa}}
    Kan använda \(H(x_1, x_2, \ldots, x_n) \leq H(x_1) + H(x_2) + \cdots 
      + H(x_n)\) för att räkna på entropin hos lösenord, där \(x_i\) är olika 
    egenskaper hos lösenordet.
  \end{block}

  \begin{example}
    \begin{itemize}
      \item Vi vill helst ha \emph{oberoende} stokastiska variabler.

      \item Vi har tillgång till exempelvis:
        \begin{itemize}
          \item längd,
          \item antal och placering för varje teckenklass,
          \item innehållet i varje tecken.
        \end{itemize}

      \item Dessa är inte oberoende.

      \item Summan av entropierna för respektive del ger \emph{övre gräns} för 
        entropin för sannolikhetsfördelningen av lösenord.

    \end{itemize}
  \end{example}
\end{frame}

\begin{frame}
  \begin{remark}
    \begin{itemize}
      \item Om variablerna ej är oberoende får vi åtminstone en övre gräns.

      \item En lägre gräns vore dock mer intresserant --- dock omöjligt att få.

      \item Denna metod används i~\cite{Komanduri2011opa} för att få en 
        realistisk uppskattning av den riktiga entropin för användarnas val av 
        lösenord.

    \end{itemize}
  \end{remark}
\end{frame}

\begin{frame}
  \begin{figure}
    \includegraphics[height=0.7\textheight]{password_strength.png}
    \caption{xkcd:s serie om lösenordsstyrka.
    Bild: xkcd~\cite{xkcd936}.}
  \end{figure}
\end{frame}

\begin{frame}{En förklaring av xkcd}
  \begin{itemize}
    \item Vi har 1 miljon engelska ord: ger \(\log 10^6 \approx 20\) bitar 
      entropi.
      (xkcd använder 16 bitar, vilket ger ca 70\,000 ord, alla ord i engelskan 
      är inte vanliga.)

    \item Vi kan ha inledande versal: ger 1 bit entropi.

    \item Vi har några vanliga substitutioner: uppskattningsvis 10 stycken, 
      d.v.s.~3 bitar entropi.

    \item Vi har specialtecken (ej substitution): uppskattningsvis 4 bitar 
      entropi.

    \item Vi har siffror: \(\log 10\approx 3\).

    \item Ordningen på specialtecknet och siffran: ger 1 bit entropi.

    \item Totalt 32 bitar entropi:
      \begin{itemize}
        \item Tar minst 50 dagar med 1\,000 gissningar per sekund.
        \item Tar strax över en timme med 1\,000\,000 gissningar per sekund.
      \end{itemize}

  \end{itemize}
\end{frame}

\begin{frame}
  \begin{example}[Ett alternativ]
    \begin{itemize}
      \item Vi har 26 bokstäver, 10 siffror och (uppskattningsvis använder vi) 
        10 specialtecken.

      \item Detta ger totalt 72 möjliga tecken: ger \(\log 72\approx 6\) bitar 
        entropi per tecken.

      \item Ett 10 tecken långt \emph{slumpmässigt valt} lösenord ger således 
        60 bitar entropi.

      \item Tar 36\,588 år med 1\,000\,000 gissningar per sekund.

    \end{itemize}
  \end{example}
\end{frame}

\begin{frame}
  \begin{example}[Ytterligare alternativ]
    \begin{itemize}
      \item Vi har 125\,000 svenska ord: ger \(\log 125\,000\approx 17\) bitar 
        entropi per ord.

      \item Ett lösenord med fyra slumpmässigt valda ord ger 68 bitar entropi.

      \item Tar 9\,359\,078 år att knäcka med 1\,000\,000 gissningar per sekund.

    \end{itemize}
  \end{example}
\end{frame}

\begin{frame}
  \begin{remark}
    \begin{itemize}
      \item Detta bygger på att vi väljer slumpmässigt.

      \item Vi är väldigt dåliga på slumpmässigt.

      \item Entropin kommer alltså att vara lägre då vi inte har en likformig 
        sannolikhetsfördelning.

    \end{itemize}
  \end{remark}
\end{frame}

\begin{frame}
  \begin{remark}
    Vad händer när vi kräver att två stora, två små och två siffror ska ingå 
    i lösenordet?
  \end{remark}
\end{frame}

\subsection{Forskning om vad som händer när människor väljer}

\begin{frame}
  \begin{itemize}
    \item \citet{Bonneau2012lpo} undersöker hur lingvistiken påverkar valet av 
      lösenord beståendes av flera ord.

    \item Finner att användarna inte väljer slumpmässiga ord utan föredrar att 
      välja dem anpassade efter naturligt språk.

    \item Exempelvis \enquote{correct horse battery staple} föredras framför 
      \enquote{horse correct battery staple} på grund av att det första 
      alternativet är mer grammatiskt korrekt.

  \end{itemize}
\end{frame}

\begin{frame}
  \begin{itemize}
    \item \citet{Kuo2006hso} gjorde en undersökning av hur användare skapar 
      lösenord som är lätta att komma ihåg.

    \item Undersökte styrkan hos frasbaserade lösenord:
      skapas utifrån en mening och förkortas.

    \item Googles exempel \enquote{To be or not to be, that is the 
        question}\footnote{%
        URL\@: 
        \protect\url{http://www.lightbluetouchpaper.org/2011/11/08/want-to-create-a-really-strong-password-dont-ask-google/}.
      } som ger lösenordet ''2bon2btitq''.

    \item I lösenordsdatabaser har just detta lösenord dykt upp \dots

  \end{itemize}
\end{frame}

\begin{frame}
  \begin{itemize}
    \item Vi kan använda läckta lösenordsdatabaser för att uppskatta entropin 
      för mänskligt valda lösenord.

    \item Vi borde också kunna uppskatta hur mycket information en given 
      lösenordspolicy ger om lösenordet genom informationsvinst.

  \end{itemize}
\end{frame}

\subsection{Identifierande information}

\begin{frame}
  \begin{example}[Vad får vi ut mest information av?]
    Får vi ut mest information av stjärntecken eller födelsedag?
    \begin{align*}
      -\sum_{\text{stjärntecken}} \frac{1}{12} \log\frac{1}{12} &= \log 12 
      \approx 3.58 \\
      &< -\sum_{\text{dagar på året}} \frac{1}{365} \log\frac{1}{365} = \log 
      365 \approx 8.51.
    \end{align*}
  \end{example}

  \pause{}

  \begin{remark}
    \begin{itemize}
      \item Om jag ska gissa er födelsedag och får veta i vilket stjärntecken 
        ni är födda gör jag en informationsvinst på ca 3.58 bitar.
      \item Då återstår ca 5 bitar att gissa, detta stämmer överens med \(\log 
        30\approx 4.91\).
    \end{itemize}
  \end{remark}
\end{frame}

\begin{frame}
  \begin{exercise}
    Hur mycket information behövs för att unikt identifiera en individ?
  \end{exercise}
\end{frame}

\begin{frame}
  \begin{example}
    \begin{itemize}
      \item Det fanns \(n = 6\,973\,738\,433\) människor på jorden, vid något 
        tillfälle under 2011 enligt Världsbanken.

      \item Att ge varje person en unik identifierare kräver \(\log 
          n = 32.7\approx 33\) bitar information.

    \end{itemize}
  \end{example}
\end{frame}

\begin{frame}
  \begin{itemize}
    \item Electronic Frontier Foundation (EFF) genomförde en 
      undersökning~\cite{Eckersley2010hui} hur mycket identifierande 
      information en webbläsare delar med sig av.

    \item Går att testa sin webbläsare på \url{http://panopticlick.eff.org/}.

    \item Med min Firefox-läsare och alla tillägg uppskattar de att jag ger 
      ifrån mig 21.45 bitar med identifierande information.

    \item De har genomfört 2\,860\,696 test totalt.

  \end{itemize}
\end{frame}

\begin{frame}
  \begin{figure}
    \includegraphics[height=0.7\textheight]{collusion.png}
    \caption{Skärmdump från Collusion (sedemera Lightbeam) för Firefox över 
    alla sidor som spårar mig med hjälp av denna information.}
  \end{figure}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%

\subsection*{References}
\begin{frame}[allowframebreaks]
	\small
  \printbibliography{}
\end{frame}

\end{document}
