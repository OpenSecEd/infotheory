\title{Elementary probability theory}
\author{Daniel Bosk}
\institute{%
  Department of Information Systems and Technology\\
  Mid Sweden University, SE-851\,70 Sundsvall
}

\begin{frame}
\maketitle
\end{frame}

\mode*

\section{Some definitions}

\subsection{Outcomes}

\begin{frame}
  \begin{definition}[Outcome]
    \begin{itemize}
      \item \emph{Sample space} \(\Omega\) of all possible outcomes.
      \item Elements \(\omega\in \Omega\) are called outcomes.
      \item An \emph{event} is a set \(A\subseteq \Omega\).
    \end{itemize}
  \end{definition}
\end{frame}

\begin{frame}
  \begin{example}
    \begin{itemize}
      \item Sample space of a roll of a die: \[\Omega_D = \{\fcdice{1}, 
          \fcdice{2}, \fcdice{3}, \fcdice{4}, \fcdice{5}, \fcdice{6}\},\]
        
        \pause{}

      \item The set of possible events is \(\powerset(\Omega_D) 
          = \{\{\fcdice{1}\}, \ldots, \{\fcdice{6}\}, \{\fcdice{1}, 
          \fcdice{2}\}, \ldots, \{\fcdice{1}, \fcdice{6}\}, 
          \{\fcdice{2},\fcdice{3}\}, \ldots, 
          \{\fcdice{1},\ldots,\fcdice{6}\}\}\).
      \item Thus \(\{\fcdice{1}\},\ldots,\{\fcdice{6}\}\) are events.
      \item But so is \(\emptyset\) and \(\{\fcdice{1}, \ldots, \fcdice{6}\}\).

        \pause{}

      \item An event means that one of its outcomes happen.
      \item So that we can express \enquote{the die roll results in an even 
          number}: \(\{\fcdice{2}, \fcdice{4}, \fcdice{6}\}\).
    \end{itemize}
  \end{example}
\end{frame}

\subsection{Probability measure}

\begin{frame}
  \begin{definition}[Probability measure]
    \begin{itemize}
      \item \(\Omega\) is a sample space.
      \item A \emph{probability measure} is a function \(\Pr\colon 
          \powerset(\Omega)\to [0,1]\) such that:
        \begin{enumerate}
          \item \(\Pr[ \emptyset ] = 0\) and \(\Pr[ \Omega ] = 1\),
          \item If \(A\subseteq \Omega\) and \(B\subseteq \Omega\) and \(A\cap 
              B = \emptyset\), then \(\Pr[A\cup B] = \Pr[A] + \Pr[B]\).
        \end{enumerate}
    \end{itemize}
  \end{definition}

  \begin{exercise}
    This is well-defined, but why?
    For instance, how do we know that \(0\leq \Pr[A] + \Pr[B]\leq 1\)?
  \end{exercise}
\end{frame}

\begin{frame}
  \begin{example}
    \begin{itemize}
      \item Let \(\Omega_D\) be the sample space for the die roll.
      \item How shall we define \(\Pr\colon \powerset(\Omega_D)\to [0, 1]\)?

        \pause{}

      \item \(\Pr[\{\fcdice{1}\}] = \cdots = \Pr[\{\fcdice{6}\}] 
          = \frac{1}{6}\)
    \end{itemize}
  \end{example}

  \pause{}

  \begin{exercise}
    The probability of the die showing an even number after a roll is 
    \(\frac{3}{6}\), why?
  \end{exercise}
\end{frame}

\subsection{Probability space}

\begin{frame}
  \begin{definition}[Probability space]
    \begin{itemize}
      \item \(\Omega\) is a sample space.
      \item \(\Pr\) is a probability measure on \(\Omega\).

        \pause{}

      \item We call \((\Omega, \Pr)\) for a \emph{probability space}.
    \end{itemize}
  \end{definition}
\end{frame}

\section{Some results}

\subsection{Complement}

\begin{frame}
  \begin{theorem}[Probability for event and complement]
    \begin{itemize}
      \item Let \(A\subseteq \Omega\) be an event and \(C = \Omega\setminus A\) 
        its complement.
      \item Then \(\Pr[C] = 1 - \Pr[A]\).
    \end{itemize}
  \end{theorem}

  \pause{}

  \begin{proof}
    \begin{itemize}
      \item Definition said \(\Pr[\Omega] = 1\).

        \pause{}

      \item \(A\cup C = \Omega\), thus \(\Pr[A\cup C] = 1\).

        \pause{}

      \item It also said that for \(A\subseteq \Omega\) and \(B\subseteq 
          \Omega\) we have \(\Pr[A\cup B] = \Pr[A] + \Pr[B]\).

        \pause{}

      \item So \(\Pr[A\cup C]\) is also \(\Pr[A] + \Pr[C]\).

        \pause{}

      \item Thus \(\Pr[A] + \Pr[C] = 1\) and so \(\Pr[C] = 1 - \Pr[A]\).
    \end{itemize}
  \end{proof}
\end{frame}

\begin{frame}
  \begin{example}
    \begin{itemize}
      \item Rolling an even number was probability \(\frac{3}{6}\).
      \item Applying the theorem, rolling an odd number is \(1-\frac{3}{6}\).
    \end{itemize}
  \end{example}

  \pause{}

  \begin{exercise}
    Give two ways to compute the probability of rolling a \fcdice{6}.
  \end{exercise}
\end{frame}

\subsection{Independent events}

\begin{frame}
  \begin{definition}[Independent events]
    \begin{itemize}
      \item \((\Omega, \Pr)\) is a probability space.
      \item Events \(A\subseteq \Omega\) and \(B\subseteq \Omega\) are 
        \emph{independent} if \(\Pr[A\cap B] = \Pr[A]\cdot \Pr[B]\).
    \end{itemize}
  \end{definition}
\end{frame}

\begin{frame}
  \begin{example}
    \begin{itemize}
      \item We roll one die, \(\Omega_D = \{\fcdice{1}, \ldots, \fcdice{6}\}\).
      \item Events: roll a one, \(A = \{\fcdice{1}\}\), and roll a six, \(B 
          = \{\fcdice{6}\}\).

        \pause{}

      \item Clearly we cannot get \fcdice{1} and \fcdice{6} at the same time:
        \[\Pr[A\cap B] = \Pr[\emptyset] = 0 \neq \Pr[A]\cdot \Pr[B].\]

      \item But we can get either: \(\Pr[A\cup B] = \Pr[A] + \Pr[B] 
          = \frac{2}{6}\).
    \end{itemize}
  \end{example}
\end{frame}

\begin{frame}
  \begin{example}
    \begin{itemize}
      \item We roll two dice: \(\Omega_{D2} = \{(\fcdice{1},\fcdice{1}), 
          \ldots, (\fcdice{1}, \fcdice{6}), \ldots, (\fcdice{6}, \fcdice{1}), 
          \ldots, (\fcdice{6}, \fcdice{6})\).

        \pause{}

      \item Roll a one: \(A = \{(\fcdice{1}, \fcdice{1}), \ldots, (\fcdice{1}, 
          \fcdice{6}), (\fcdice{2}, \fcdice{1}), \ldots, (\fcdice{6}, 
          \fcdice{1})\}\).

      \item Roll a six: \(B = \{(\fcdice{6}, \fcdice{1}), \ldots, (\fcdice{6}, 
          \fcdice{6}), (\fcdice{1}, \fcdice{6}), \ldots, (\fcdice{5}, 
          \fcdice{6})\}\).

        \pause{}

      \item Roll a one and a six: \(\Pr[A\cap B] = \Pr[\{(\fcdice{1}, 
          \fcdice{6}), (\fcdice{6}, \fcdice{1})\}] = \frac{2}{36} \neq 
          \frac{1}{6}\cdot \frac{1}{6}\).

      \item Roll a one or a six with two chances: \(\Pr[A\cup B]\).

    \end{itemize}
  \end{example}
\end{frame}

\begin{frame}
  \begin{example}
    \begin{itemize}
      \item We roll two dice: \(\Omega_{D2} = \{(\fcdice{1},\fcdice{1}), 
          \ldots, (\fcdice{1}, \fcdice{6}), \ldots, (\fcdice{6}, \fcdice{1}), 
          \ldots, (\fcdice{6}, \fcdice{6})\).

        \pause{}

      \item \emph{First} roll a one: \(A = \{(\fcdice{1}, \fcdice{1}), \ldots, 
          (\fcdice{1}, \fcdice{6})\}\).

      \item \emph{Second} roll a six: \(B = \{(\fcdice{1}, \fcdice{6}), \ldots, 
          (\fcdice{6}, \fcdice{6})\}\).

        \pause{}

      \item First roll a one \emph{and} then a six: \(\Pr[A\cap B] 
          = \Pr[\{(\fcdice{1}, \fcdice{6})\}] = \frac{1}{36} = \frac{1}{6}\cdot 
          \frac{1}{6}\).

      \item Either roll a one first \emph{or} a six on the second: \(\Pr[A\cup 
          B]\).

    \end{itemize}
  \end{example}
\end{frame}

%\begin{frame}
%  \begin{exercise}
%    Even if \(\Pr[A]\cdot \Pr[B] = \Pr[A\cap B]\) they are not necessarily 
%    independent, why?
%  \end{exercise}
%\end{frame}

\section{Conditional and joint probability}

\subsection{Conditional probability}

\begin{frame}
  \begin{definition}[Conditioned probability]
    \begin{itemize}
      \item Let \(A, B\) be events in probability space \((\Omega, \Pr)\).
      \item The probability of \(B\) given \(A\) is defined as \(\Pr[B\mid A] 
          = \frac{\Pr[B\cap A]}{\Pr[A]}\)
    \end{itemize}
  \end{definition}

  \pause{}
  
  \begin{example}
    \begin{itemize}
      \item Use probability space for rolling a die, \((\Omega_D, \Pr)\).
      \item Event \(E = \{\fcdice{2}, \fcdice{4}, \fcdice{6}\}\) for even side 
        up.

        \pause{}

      \item Let \(A = \{\fcdice{2}\}\) and \(B = \{\fcdice{3}\}\), then
        \[\Pr[A\mid E] = \frac{\Pr[\{\fcdice{2}\}]}{\Pr[E]} = \frac{1}{3}
          \text{ and }
          \Pr[B\mid E] = \frac{\Pr[\emptyset]}{\Pr[E]} = 0.\]

    \end{itemize}
  \end{example}
\end{frame}

\subsection{Joint probability}

\begin{frame}
  \begin{theorem}[Joint probability]
    \begin{itemize}
      \item Let \(A, B\) be events in probability space \((\Omega, \Pr)\).
      \item Then \(\Pr[A\cap B] = \Pr[A]\cdot \Pr[B\mid A]\)
    \end{itemize}
  \end{theorem}

  \pause{}

  \begin{exercise}
    Using conditional probability we can show this new result, how?
  \end{exercise}
\end{frame}

\begin{frame}
  \begin{proof}
    \begin{itemize}
      \item We know from definition: \(\Pr[B\mid A] = \frac{\Pr[A\cap 
            B]}{\Pr[A]}\).

      \item Thus \(\Pr[B\mid A]\cdot \Pr[A] = \Pr[A\cap B]\).
    \end{itemize}
  \end{proof}
\end{frame}

\subsection{Bayes' theorem}

\begin{frame}
  \begin{theorem}[Bayes' theorem]
    \begin{itemize}
      \item Let \(A, B\) be events in probability space \((\Omega, \Pr)\).
      \item Then \(
          \Pr[A\mid B] = \frac{\Pr[B\mid A]\Pr[A]}{\Pr[B]}.
        \)
    \end{itemize}
  \end{theorem}

  \pause{}

  \begin{proof}
    \begin{itemize}
      \item \(\Pr[A\mid B] = \frac{\Pr[A\cap B]}{\Pr[B]}\) and \(\Pr[B\mid A] 
          = \frac{\Pr[A\cap B]}{\Pr[A]}\).

        \pause{}

      \item Thus \(\Pr[A\cap B] = \Pr[A\mid B] \Pr[B] = \Pr[B\mid A] \Pr[A]\).

        \pause{}

      \item Which means \(\Pr[A\mid B] = \frac{\Pr[B\mid A] \Pr[A]}{\Pr[B]}\).
    \end{itemize}
  \end{proof}
\end{frame}

\begin{frame}
  \begin{example}[An intrusion detection system]
    \begin{itemize}
      \item Our IDS is this good:
        \begin{itemize}
          \item It detects malicious traffic with \SI{99}{\percent} certainty 
            (true positives).
          \item It discards benign traffic with \SI{99}{\percent} certainty 
            (true negatives).
        \end{itemize}

        \pause{}

      \item \emph{Assume} \SI{0.5}{\percent} of \enquote{the traffic} consist 
        of attacks.

        \pause{}

      \item Event \(M\) is malicious traffic, \(B\) is benign traffic.
      \item Event \(+\) means IDS declared malicious, \(-\) declared benign.

        \pause{}

      \item Then we get \dots
    \end{itemize}
  \end{example}
\end{frame}

\begin{frame}
  \begin{example}[IDS continued]
    \begin{align*}
      \Pr[M\mid +] &= \frac{\Pr[+\mid M] \Pr[M]}{\Pr[+]}
      = \frac{0.99\cdot 0.005}{\Pr[+]} \\
      &= \frac{0.99\cdot 0.005}{\Pr[+\mid M] \Pr[M] + \Pr[+\mid B] \Pr[B]} 
      \\
      &= \frac{0.99\cdot 0.005}{0.99\cdot 0.005 + 0.01\cdot (1 - \Pr[M])} 
      \\
      &= \frac{0.99\cdot 0.005}{0.99\cdot 0.005 + 0.01\cdot 0.995} \\
      &= 0.33
    \end{align*}
  \end{example}
\end{frame}

\begin{frame}
  \begin{exercise}
    \begin{itemize}
      \item What's a good-enough probability for \(\Pr[M\mid +]\)?
      \item How do we achieve it?
    \end{itemize}
  \end{exercise}
\end{frame}


