The area of Information Theory was founded in 1948 by Claude Shannon.
It concerns information, e.g.\ how much information is contained in certain 
data.
Equivalently, it is also a measure of uncertainty in information, and has thus 
plenty of application in security and cryptography.

The concept of entropy, the main part of information theory, is treated in 
a few short texts: \citetitle{Eckersley2010apo}~\cite{Eckersley2010apo} and 
applied in \citetitle{Eckersley2010hui}~\cite{Eckersley2010hui}, but also in 
\citetitle{Ueltschi2013se}~\cite{Ueltschi2013se}.
This is then utilised in the text \citetitle{Bosk2013gl}~\cite{Bosk2013gl} (in 
Swedish), and \citetitle{Komanduri2011opa}~\cite{Komanduri2011opa} which treats 
passwords.
